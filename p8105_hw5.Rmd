---
title: "P8105_hw5"
output: github_document
date: "2022-11-04"
---
# libraries
```{r include=FALSE}
library(tidyverse)
library(ggplot2)
library(patchwork)
```

# Problem 1
## Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time
```{r warning=FALSE, message=FALSE}
file_df <- tibble(
  files = list.files("./data")) %>%
  mutate(files = str_c("data", files, sep = "/"))

results <- file_df %>%
  mutate(results = map(files, read_csv)) %>%
  mutate(arm = ifelse(str_detect(files, "exp"), "Experiment", "Control"),
         ID = as.factor(parse_number(files))) %>%
  unnest(results) %>%
  pivot_longer(week_1:week_8,
               names_to = "week",
               values_to = "observation")%>%
  mutate(week = as.numeric(parse_number(week))) 
head(results) %>%
  knitr::kable()
```

 
Make a spaghetti plot showing observations on each subject over time
```{r}
results %>%
  ggplot(aes(x = week, y = observation,color = ID)) + geom_line() +
  ylab("Observations") +
  xlab("Weeks") +
  facet_grid(cols = vars(arm)) +
  ggtitle("Observations Over time by Arm")
```

The Charts shows that the observations over time by study arm. In the control arm, we cannnot say there is a relationship between participants' observation and time. However,in the experiment arm, participants appear to be reporting higher observations as time goes on.

 
# Problem 2
## read data from website
```{r, warning=FALSE, message=FALSE}
url <- 'https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv'
homicides_df <- read_csv(url(url), na = c(" ", "Unknown"))
skimr::skim(homicides_df)
```

The homicide data set contains 52,179 observations across 12 columns. Variables in the data set include unique id,  victim's last name, victim's first name, victim's race, victim's age, victim's sex, city, state, latitude, longitude, reported date of murder and disposition of the case.


## Create city_state variable and summarize within cities to obtain the total number of homicides and the number of unsolved homicides
```{r}
# create  new variables
homicides_df <- homicides_df %>% 
  mutate(city_state = str_c(city, state, sep = ", "),
         whether_solved = ifelse(
           disposition %in% c("Closed without arrest", "Open/No arrest"), "unsolved", "solved"))
 
```

## Filter to Baltimore
```{r}
baltimore <- homicides_df %>%
  filter(city_state == "Baltimore, MD")


unsolved_baltimore_summary <- baltimore %>%
  summarize(
    unsolved = sum(whether_solved == "unsolved"),
    n=n()
  )

test <-prop.test(
  x = unsolved_baltimore_summary %>% pull(unsolved),
  n = unsolved_baltimore_summary %>% pull(n)
)

test %>%
  broom::tidy()
```
## run prop.test in other cities
```{r warning=FALSE}
# table for each city
city_df <- homicides_df %>%
  group_by(city_state) %>%
  filter(city_state != "Tulsa, AL") %>% #Tulsa is not in AL
  summarise(
    unsolved = sum(whether_solved == "unsolved"),
    n = n()) 

results_df = 
  city_df %>% 
  mutate(
    prop_tests = map2(.x = unsolved, .y = n, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low,conf.high) 
  
results_df %>%
  knitr::kable()
```
## Create a plot that shows the estimates and CIs for each city 
```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


# Problem 3
## Set μ=0
```{r}
set.seed(100)
sim = function(n=30, mu, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
  sim_data %>% 
    t.test(mu = 0, conf.level = 0.95) %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
}

# Generate 5000 datasets and apply the function
output_list =
  rerun(5000, sim(30, 0, 5)) %>%
  bind_rows()
output_list
```


## Repeat the above for μ={1,2,3,4,5,6}
```{r}
sim_results = 
  tibble(mu = 1:6) %>% 
  mutate(
    output_lists = map(.x = mu, ~rerun(5000, sim(mu = .x))),
    estimate_dfs = map(output_lists, bind_rows)) %>% 
  select(-output_lists) %>% 
  unnest(estimate_dfs)
```

## Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.
```{r warning=FALSE}
sim_results %>%
  group_by(mu) %>%
  summarize(n = n(),
            power = sum(p.value < .05)/n()*100) %>%
ggplot(aes(x = mu, y = power)) + 
  geom_point() + 
  geom_smooth(se = FALSE) + 
  labs(x = "Value of mu", y = "Proportion Null Rejected", title = "Proportion of Null rejected by Mu")
 
```  
We can see as the effect size increases,the power increases. But the increasing rate gets smaller.

## Make a plot showing the average estimate of μ^ on the y axis and the true value of μ on the x axis. Make a second plot (or overlay on the first) the average estimate of μ^ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis.
```{r}
# First plot
plot1 <- sim_results %>%
  group_by(mu)%>%
  summarize(
    avg_mu_hat = mean(estimate),
    ) %>%
  ggplot(aes(y = avg_mu_hat, x = mu)) +
  geom_point()+
  geom_path()+
  labs(
    x = "True mu",
    y = "Average estimate of mu_hat"
  )

plot2 <- sim_results %>% 
      filter(p.value<0.05) %>% 
      group_by(mu) %>% 
      summarise(
        avg_mu_hat_rej = mean(estimate)
      )%>%
  ggplot(aes(y = avg_mu_hat_rej, x = mu)) +
  geom_point()+
  geom_path()+
  labs(
    x = "True mu",
    y = "Average estimate of mu_hat which in samples reject the null"
  )
plot1 + plot2
```

The sample average for which the null is rejected is approximately equal to the true value of mu when effect power of the test is large, when power is small, true mu is always smaller than average estimate. This is because the power of test increases with the increase in effect size. So the average estimate of mu_hat that reject the null will become closer to true mu as the effect size increases.








 